{
  "cells":[
    {
      "cell_type":"markdown",
      "source":[
        "# Data Cleaning for NLP Project\n",
        "Please donot delete other's code, and test new codes at the end of the notebook before adding to the final cell"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Extract Script from Scraps From The Loft"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### Using Beautiful Soup and Web Scraping"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def get_links(url):\n",
        "    page = requests.get(url).text\n",
        "    soup = BeautifulSoup(page, \"lxml\")\n",
        "    links = {tag.a.text.strip() : tag.a.get('href') for tag in soup.find_all(class_=\"elementor-post__title\")}\n",
        "    return links"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def get_scripts(links):\n",
        "    for key, url in tqdm(links.items()):\n",
        "        page = requests.get(url).text\n",
        "        soup = BeautifulSoup(page, 'lxml')\n",
        "        transcript = soup.find(class_='elementor-element elementor-element-74af9a5b elementor-widget elementor-widget-theme-post-content').find_all('p')\n",
        "        transcript = \"\\n\".join([i.text for i in transcript])\n",
        "        links[key] = [transcript]\n",
        "    return links"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### Save Scripts as dictionary and dataframe"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import pickle\n",
        "pickle.dump(scripts, open(\"scripts.pickle\", \"wb\"))"
      ],
      "execution_count":1,
      "outputs":[
        {
          "ename":"NameError",
          "evalue":"NameError: name 'scripts' is not defined",
          "traceback":[
            "\u001b[0;31m---------------------------------------------------------------------------",
            "Traceback (most recent call last)",
            "    at line 2 in <module>",
            "NameError: name 'scripts' is not defined"
          ],
          "output_type":"error"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import pandas as pd\n",
        "x = {i : [j] for i, j in scripts.items()}\n",
        "df = pd.DataFrame.from_dict(x, orient='index')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "df = df.reset_index()\n",
        "df = df.rename(columns=dict(zip(df.columns, ['Title', 'Transcript'])))\n",
        "df.head()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "pickle.dump(df, open('transcript.pickle', 'wb'))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "df.to_csv('transcript.csv')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Extract Script from Youtube using Youtube API"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "!pip install google-api-python-client\n",
        "!pip install youtube_transcript_api\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "from apiclient.discovery import build\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "from collections import defaultdict\n",
        "def get_video_ids(apikey, artists=None):\n",
        "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "    transcript = defaultdict(list)\n",
        "    for comedian in artists:\n",
        "        print(comedian)\n",
        "        for count in range(10):\n",
        "            req =  youtube.search().list(part='snippet', type='video', q=comedian+' Stand Up', maxResults=50).execute()\n",
        "            nextPage = req['nextPageToken']\n",
        "            for i in req['items']:\n",
        "                channel = i['snippet']['channelTitle']\n",
        "                videoID = i['id']['videoId']\n",
        "                videoTitle = i['snippet']['title']\n",
        "                transcript[comedian].append((videoID, videoTitle))\n",
        "    \n",
        "    return transcript\n",
        "        \n",
        "\n",
        "api_key = 'AIzaSyCwJIeVW_iKY74omPM9RqsBjgJbKf1ANF8'\n",
        "api_hritika = 'AIzaSyCh0PvO4TlSkEhE3mfnQsafm4aBTS-sO20'\n",
        "artists = ['Kenny Sebastian', 'Trevor Noah', 'Hasan Minhaj', 'John Mulaney', \n",
        "           'Kevin Hart', 'Dave Chappelle', 'Amy Schumer', 'Kanan Gill', \n",
        "           'Vir Das', 'Zakir Khan', 'Aditi Mittal', 'Ali Wong']\n",
        "\n",
        "transcript_ID = get_video_ids(api_key, artists)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "for i, j in transcript_ID.items():\n",
        "    print(i, len(j))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Get the Transcripts\n",
        "https:\/\/pypi.org\/project\/youtube-transcript-api\/"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "!pip install youtube_transcript_api\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from tqdm import tqdm\n",
        "import pickle"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def get_transcripts(transcipt_ID, index=0):\n",
        "    transcripts = dict()\n",
        "    for i in list(transcript_ID.keys())[index:index+1]:\n",
        "        transcripts[i] = list()\n",
        "        for video_id, title in tqdm(transcript_ID[i]): \n",
        "            try:\n",
        "                req = YouTubeTranscriptApi.get_transcript(video_id, languages=['en', 'en-US', 'en-UK'])\n",
        "                transcripts[i].append((title, req))\n",
        "            except:\n",
        "                continue\n",
        "    return transcripts"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "transcript_ID = pickle.load(open(\"transcript_ID.pickle\", \"rb\"))\n",
        "print(*enumerate(transcript_ID), sep='\\n')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "transcripts = get_transcripts(transcript_ID, index=11)\n",
        "print(\"\\n\", transcripts.keys(),len(transcripts[list(transcripts.keys())[0]]))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "pickle.dump(transcripts, open(\"Ali_Wong.pickle\", \"wb\"))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "artist = ['Kenny Sebastian', 'Trevor Noah', \n",
        "          'Hasan Minhaj', 'John Mulaney', \n",
        "          'Kevin Hart', 'Dave Chappelle', \n",
        "          'Amy Schumer', 'Kanan Gill', \n",
        "          'Vir Das', 'Zakir Khan', \n",
        "          'Aditi Mittal', 'Ali Wong']"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "x = list()\n",
        "for i in list(map(lambda x : x.replace(\" \", \"_\"), artist)):\n",
        "    x.append(pickle.load(open(i+\".pickle\", \"rb\")))\n",
        "\n",
        "artists_transcript = dict()\n",
        "for i, j in zip(x, artist):\n",
        "    artists_transcript[j] = i[j]\n",
        "pickle.dump(artists_transcript, open(\"12Artists.pickle\", \"wb\"))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Combine Transcripts and Save"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import pandas as pd\n",
        "import pickle"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "transcripts = pickle.load(open(\"12Artists.pickle\", \"rb\"))\n",
        "final = list()\n",
        "for artist in transcripts.keys():\n",
        "    for script in range(len(transcripts[artist])):\n",
        "        title, text = transcripts[artist][script]\n",
        "        text = \" \".join([i['text'] for i in text])\n",
        "        final.append((artist, title, text))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "df = pd.DataFrame(final, columns=['Artist', 'Title', 'Transcript'])"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "df.head()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "df1 = pd.read_csv(\"transcript Part-1.csv\").drop(\"Unnamed: 0\", axis=1)\n",
        "df2 = pd.read_csv(\"transcript Part-2.csv\").drop(\"Unnamed: 0\", axis=1)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "df1['Artist'] = df1['Title']"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "df1 = df1[['Artist', 'Title', 'Transcript']]"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "df = pd.concat([df1, df2]).reset_index().drop(\"index\", axis=1)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "df.to_csv(\"Transcript.csv\")"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "df.head()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "df.to_excel(\"Transcript.xlsx\")"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "pickle.dump(df, open(\"Transcript.pickle\", \"wb\"))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Data Cleaning for NLP Project\n",
        "Please donot delete other's code, and test new codes at the end of the notebook before adding to the final cell"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import pandas as pd\n",
        "data = pd.read_pickle(\"Transcript.pickle\")"
      ],
      "execution_count":25,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "data.head()\n",
        "# You can resize the column width by dragging over the column border, just like MS Word"
      ],
      "execution_count":26,
      "outputs":[
        {
          "data":{
            "text\/html":[
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "<\/style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th><\/th>\n",
              "      <th>Artist<\/th>\n",
              "      <th>Title<\/th>\n",
              "      <th>Transcript<\/th>\n",
              "    <\/tr>\n",
              "  <\/thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0<\/th>\n",
              "      <td>Chris Rock Total Blackout: The Tamborine Exten...<\/td>\n",
              "      <td>Chris Rock Total Blackout: The Tamborine Exten...<\/td>\n",
              "      <td>[Jimmy Fallon] Were you at the, uh, White Hous...<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>1<\/th>\n",
              "      <td>Bo Burnham: Words, Words, Words (2010) – Trans...<\/td>\n",
              "      <td>Bo Burnham: Words, Words, Words (2010) – Trans...<\/td>\n",
              "      <td>(Cheers and applause)\\nThank you.\\n(Laughter)\\...<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>2<\/th>\n",
              "      <td>Vir Das: Outside in – The Lockdown Special (20...<\/td>\n",
              "      <td>Vir Das: Outside in – The Lockdown Special (20...<\/td>\n",
              "      <td>[soft piano music playing]\\n[Vir Das] What you...<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>3<\/th>\n",
              "      <td>Larry the Cable Guy – Remain Seated (2020) – T...<\/td>\n",
              "      <td>Larry the Cable Guy – Remain Seated (2020) – T...<\/td>\n",
              "      <td>[Announcer] Ladies and gentlemen, Larry, The C...<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>4<\/th>\n",
              "      <td>Craig Ferguson: Just Being Honest (2015) – Tra...<\/td>\n",
              "      <td>Craig Ferguson: Just Being Honest (2015) – Tra...<\/td>\n",
              "      <td>Watch the full show for free on YouTube\\n[bagp...<\/td>\n",
              "    <\/tr>\n",
              "  <\/tbody>\n",
              "<\/table>\n",
              "<\/div>"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "print(\"{:10s}{}\\n{:10s}{}\".format(\"Columns:\", data.columns.values, \"Shape:\", data.shape))"
      ],
      "execution_count":27,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "Columns:  ['Artist' 'Title' 'Transcript']\n",
            "Shape:    (2071, 3)\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "print(data.iloc[2,2])"
      ],
      "execution_count":4,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "[soft piano music playing]\n",
            "[Vir Das] What you’re about to watch wasn’t supposed to happen. It’s completely unscripted. It was totally unplanned. It’s just… a moment in time. When the world shut down, we decided to do 30 shows for charity, just to raise money for COVID relief. And every night, I’d ask people the same question. What’s the first thing you’re gonna do, the first thing, when this world reopens? Welcome to lockdown day. I don’t know what day it is. That’s the thing about the entire world going through the same thing at the same time. I guess it’s special. All right. What’s up! What’s up, everyone? Hello! Good evening, and welcome to what promises to be the strangest stand-up comedy gig that you and I have both ever been a part of. If you’re not using earphones, what I ask is that you get as close to the device as you possibly can so that I can hear you laugh. Because if I cannot hear you laugh, then it’s just me in a room talking to myself, worried that I’m going to die. We’ll begin with India’s official unity mantra against the coronavirus. On the count of three, everybody here will say, “Go, corona, go, corona, go, corona, go.” All right. Three, two, one… Go!\n",
            "[audience] Go, corona, go, corona, go, corona, go. I’m not sure how this is going to go. Zoom shows, the future of stand-up… or the prelude to the death. What’s the first thing you’re going to do once the lockdown ends?\n",
            "Where’s the girlfriend?\n",
            "Santacruz. You look very young. You look like you’ve just read Harry Potter.\n",
            "[audience laughing]\n",
            "Like you’ve just finished the last page of Deathly Hallows. How old are you? Seventeen. How long has it been since you saw the girlfriend? Two months. Two months! That’s like, way before the virus. I think she might’ve left you, buddy, like… Just saying.\n",
            "That’s before the lockdown. -[audience laughing] She attended your show, er, two days ago on the 15th. So I got a recommendation from your girlfriend. That sounds wrong. No, no…\n",
            "[audience laughing]\n",
            "Comedically, f*ck you, you tharki people, all right? I do not interact with 17-year-old girls ever, on policy, under any circumstances.\n",
            "[Sargam] Celebrate my birthday.\n",
            "Sargam, where are you joining us from?\n",
            "[Sargam] Chicago.\n",
            "Chicago? Jesus! What time is it there, darling?\n",
            "[Sargam] Oh, it’s morning time. It’s 9:34.\n",
            "9:34… I mean, where the f*ck do you have to be, right?\n",
            "So, that’s great.\n",
            "[audience laughing]\n",
            "Americans, you wake up really early. They get to work by 8 o’clock over there, right? That’s how they’re the most powerful country… Er… Were the most powerful country in the world. Er… Before… Before somebody ate a bat… [all laughing] …and crashed your economy. A bat f*cked your country. Your whole country from across the world. And you thought a butterfly had a big effect. [laughing] That’s the toughest thing about this virus. It’s like we don’t know who to blame. ‘Cause that’s like the hallmark of being Indian, is before you solve a problem, you blame it on somebody else. And this is new territory for us. This is the first thing we’ve experienced in the last 73 years that is not Pakistan’s fault.\n",
            "[audience laughing]\n",
            "We are not used to things not being Pakistan’s fault. Now, all our politicians are on TV in masks. It’s great. It’s great to see conservative Hindu leaders dressing up exactly like the Muslim women they oppress.\n",
            "[audience laughing]\n",
            "We’re allowed to meet one person from outside your family, out in a park, with two-meter distance.\n",
            "[all laughing]\n",
            "Who are you utilizing this rule for? Who’s that person you’re meeting? -That’s my girlfriend. -Your girlfriend? [all laughing] And when you… When you meet your girlfriend, your steady girlfriend, after six weeks… in a park… er… you intend to socially distance? I highly doubt that. That’s why I am not gonna meet her until the lockdown is over. Can you ima… Like, I’m just picturing two people who are desperate and horny, standing two meters away from each other. And his girlfriend just going, “Man, I wish your dick was bigger right now, I really do.” “That would be so much more useful.” Eat food from my favourite place. “Eat food.” Okay, great. So, Srushti Shah, er… You know, I’m glad you completed the sentence. You know what I mean? Where you were like, “Eat food,” and we all got worried about you. And then you were like, “No, from my favourite place.” We were like, “She’s privileged. It’s fine.” We all got it then. It is okay. Yo, what are you studying? Fashion design. Fashion design. Okay, cool. We’ll pretend that’s a course. No, I’m joking. I’m joking.\n",
            "In Gandhinagar?\n",
            "Yeah. Good. You know, Gandhiji was known for great fashion. Er…\n",
            "[audience laughing]\n",
            "He had that one outfit that was really popular, you know. And he designed it himself, and, you know, it’s… Okay. Did you hear that uncomfortable silence, Srushti? On the Zoom call. All right. Cool. Erm…\n",
            "I love Asian food.\n",
            "Okay. The place is called TG’s. That’s the first place I’m going for. You’re eating dal-sabzi and all at home?\n",
            "Haan.\n",
            "You know that’s Asian food.\n",
            "[audience laughing]\n",
            "Oriental food. I’m just putting it out there, like… Cool. You realize we’re not in Scotland right now. Technically, this is… [laughs] I may be in a place that the virus came from, normally. You live… You’re from Wuhan, really? No, no, no. Are you patient zero? You brought it over? Normally, I live and I work in China. I do think that it’s a lot worse than maybe what they’ve let on. No. I mean, China is just like, “No, that’s not a body, that’s a…” “He’s just sleeping,” right? That’s basically all China’s been doing. “We’ve absolutely no new cases.” [imitates gunshots]\n",
            "[Upasna] Graduate.\n",
            "Upasna, graduate in what? Architecture from Manipal University. Architecture from Manipal University, which is half real. Let’s be honest, all right.\n",
            "[audience laughing]\n",
            "The field of architecture is real, but Manipal University means, “Papa gave 20 lakhs in a suitcase to somebody.” That’s basically what that shit is, right? There are places in Gandhinagar where you can go. NIFT, Gandhinagar. NIFT, Gandhinagar, cool. There’s… Er… The National Institute of Fashion Technology, right?\n",
            "That’s what NIFT is?\n",
            "Yes. ‘Cause, you know, fashion is all about technology. Er…\n",
            "[audience laughing]\n",
            "You know, sometimes… [laughs] Sometimes, I’ll put on a T-shirt and I’ll be like, “My God, this is hi-tech.” You know… “This is just…” “Whoever designed this…” [chuckles] “You know, when I eat Asian food, it just slides right off.” “How did they make that happen with this T-shirt?” I think I made fun of the fashion girl a little too much. Like, no stylist is ever gonna work with me again. It’s okay. Isn’t it all just hazmat suits for the next f*ckin’ decade? “Asian food.” It’s… I had to, I’m sorry.\n",
            "[man] Celebrate 420.\n",
            "Oh, man. Don’t you have ganja?\n",
            "[man] No, bro. So, every time I go to a college, people write in there, “Vir Das. ‘Babaji ki Booty.’ Do you wanna get high together, bro?” “You want to smoke a ‘J’ together, bro?” And I’m like, “No!” Can you imagine what would happen if you, a college kid, got high with me, a 40-year-old man? And you’re like… [inhales deeply] “Hey, Vir Das, do you feel like the moon landing was fake and they never told us about that shit?”\n",
            "[audience laughing]\n",
            "You know, and I’d be like, “I have acid reflux.” Like, you know… [laughs] It would just be such a sad day. Are you able to smoke in your house? Yeah. Late night, I sneak up to my terrace and do things when they’re sleeping. You realize they know, right? They’re in their bed, like… [sniffs] “That’s Manali.” That’s what’s happening in your house right now. What’s the first thing you’re going to do after the lockdown ends? Somebody yell it out.\n",
            "[Tanay] Stay home.\n",
            "Who the f*ck is that? What are you sitting on? Is that a video gamer’s chair? Is that what that is?\n",
            "That’s a gamer’s chair, yeah.\n",
            "That’s a gamer’s chair, yeah.\n",
            "[audience laughing]\n",
            "Just have some multivits, buddy, because… I mean… Gamer generation, you guys… You can’t survive a peanut allergy. I’m not confident of your abilities in a pandemic. I am sorry. If you had to go outside, where would you go, what was your thing?\n",
            "Clubbing.\n",
            "What’s your favourite club? It was Tryst, but then it shut down for renovation. Yeah, bro, I feel you. Tryst… [all laughing] Tryst. Tryst was lit AF. You know, like, just… You know, like, f*ckin’ Friday night. Have like a… A Rooh Afza shot and build a world and… Head out to Tryst.\n",
            "[woman] I’m from Dubai, UAE.\n",
            "You’re from UAE. That’s not really, yaar… That’s just…\n",
            "[audience laughing]\n",
            "That’s just India with cleaner and meaner people. If India loved architecture more and hated Indians more, that’s Dubai.\n",
            "[audience laughing]\n",
            "So lockdown has been extended, again. And it will probably be extended again. And again… And again, and again, and… What’s the first thing you guys are gonna do after the lockdown ends? Yell it out.\n",
            "[man] I’ll have ice cream.\n",
            "Is it not available where you are?\n",
            "[audience laughing]\n",
            "I think it’s not available anywhere. Because what we’re doing here, I don’t know if you guys are doing this… Like, the package comes and then we sanitize the package. Er, and then, we take the food and immediately put it in another container and microwave that shit for 30 seconds to nuke all the f*ckin’… So just do that with your ice cream. Have some soup, man. [laughs] I don’t know what else to…\n",
            "[audience laughing]\n",
            "Pranav, you’re in Karnataka. What do you do, buddy? Er, actually, I was supposed… I’m supposed to go to US for my studies. I am just waiting.\n",
            "[woman] Oh… Pranav, I have bad news.\n",
            "[Pranav chuckles]\n",
            "[woman] Aw… Er… And… But you have the visa and everything, it’s all done? No, no. Only I have admission and I have the date.\n",
            "[Pranav chuckles]\n",
            "[women] Aw… I hope it happens, man. I really, really do.\n",
            "Like, I’m f*ckin’…\n",
            "Thanks. I’m rooting for you. Like, positive energy from me to you. It’s completely unrealistic, but still, from me to you.\n",
            "Yes.\n",
            "[audience laughing]\n",
            "[Vir Das] I know what it is to have that feeling, you know, firsthand. That feeling of, er… “I wanna go to America. My life’s gonna change.” “Everything’s gonna be…” I hope this kid makes it there before this virus kills that feeling. ‘Cause that feeling is important. [chuckles] I was feeling sorry for myself before the show. This kid is going through that. He just wants ice cream. That’s what he wants. Ice cream. And I need to toughen the f*ck up and… keep doing this.\n",
            "What’s your name?\n",
            "Hillary.\n",
            "Hillary?\n",
            "[Hillary] Yeah, from Moscow.\n",
            "Hillary from Moscow, really? Oh, so… Oh, wow… So you are Mr. Hillary Dsouza. Good to see you, sir. That’s right. That’s right. That was not the surname that I was expecting when I heard “Hillary.”\n",
            "[all laughing]\n",
            "I’m gonna be very honest, I thought of an unhappy wife. Er… Millennials, that’s a Hillary Clinton joke, never mind. We’ll get to it. It’s the news.\n",
            "[indistinct chatter]\n",
            "It’s fine. No, no, no… She didn’t win. She doesn’t deserve your laughter. [Hetika] Go attend my dance class. Non-virtual. Hetika. What’s up, Hetika? What kind of dance? Erm, so I’m learning bhangra currently. Yo, lady. You don’t have to learn bhangra.\n",
            "I don’t think there’s bhangra classes. -[audience laughing] There are. A more formal way of it. But, yeah. What happens in bhangra class? Er, sastriyaakaal, welcome to… [laughs] [Hetika] That. Just that and– Welcome to bhangra class. There are two buttons on ceiling. Hit buttons.\n",
            "I don’t know what the…\n",
            "Absolutely. For some reason, Indians do bhangra to classic rock.\n",
            "I don’t know if you’ve like–\n",
            "Oh, God. That’s horrible. They’re like, “‘November Rain,’ bhenchod.” Like, you know… It’s just…\n",
            "[audience laughing]\n",
            "I love it when we make white people do bhangra, ’cause they’re so happy to be included, right? They’re just like… “I think they forgave us.”\n",
            "[audience laughing]\n",
            "[woman] Houston, Texas.\n",
            "Houston, Texas. What time is it there? 8:30 in the morning. Jesus Christ! [laughs] You’re watching stand-up for breakfast? Are you f*ckin’ serious? I’ve never had a gig where somebody eats poha in the middle of the gig. That’d be f*ckin’ great. Did you see Donald Trump today? It was great. He said, “You can inject disinfectant into your body.” If you inject disinfectant into your body, you don’t have to worry about the coronavirus. You know what I’m saying? Or even cleaning your house anymore. You have other worries, like, you know… Flowers, food, the size of the coffin, you know. Other things that you need to… to think about, man. What a train wreck, your guy is. But I’m really sorry. But, I mean, at least you have, like, some 500 trillion dollars or whatever, right? At least, you know… At least Donald Trump’s plan is not…\n",
            "[imitates Trump] “Er, I want everyone to go to the balcony at 8:00 p.m.” Like, at least that’s not the plan.\n",
            "[all laughing]\n",
            "Can you imagine if that was Donald Trump’s plan…\n",
            "[imitates Trump] “Er, we have the best balconies and I want everybody to go to the balcony at 8:00 p.m. and light a candle.” “We have the best candles in America.” “God bless America, God bless American candles.” “Good night, we’re all gonna die.” Like if that was… [laughs] Er, what’s the first thing you’re gonna do once the lockdown ends? Somebody yell it out.\n",
            "[Ashish] Haircut.\n",
            "Ashish, you look all right, though. -Like, it’s…\n",
            "[Ashish] No, like… If you look at my beard, it’s too much than what I have normally. So, yeah. Beard and hair. I mean, we’re all being polite right now, and ignoring the fact that you need somebody else to trim your beard. Which is…\n",
            "[audience laughing]\n",
            "Just the most privileged f*cking thing in the world. The thing is that I actually used to do that– “No, but Ramu does it so much better.”\n",
            "[audience laughing]\n",
            "“Ramu gets in there, where I don’t want to go.” Barber… When I go to the barber– Shut up. Don’t explain your privilege, own it. [audience laughing] It’s either been, like, close to two months or 16 years since the lockdown began and… We took a little bit of a break from doing the shows because I was going through… some stuff. And now we are back. People come and see a Vir Das show, they come to forget about their shit. They don’t come to see mine. So… put your shit away.\n",
            "♪ Put your shit away… ♪\n",
            "[woman] So, it’s 6:50 a.m. right now.\n",
            "6:50 in the morning. You got up to see a comedy show with your boyfriend?\n",
            "Oh, wow.\n",
            "So, marriage maybe after this? [chuckles] Oh, my God. Okay, it’s too early for this. “It’s too early for this,” as in 6:50 in the morning or in life? What are we talking about exactly?\n",
            "Is it like… What’s the problem? Is it…\n",
            "I should go.\n",
            "[in Hindi] “I haven’t graduated” or “I haven’t taken a dump”?\n",
            "[in English] What is the actual problem?\n",
            "[man] Go back to Canada.\n",
            "Go back to Canada.\n",
            "Aw…\n",
            "[audience laughing]\n",
            "Me and my girlfriend, she’s on the call here. We came here for our engagement ceremony. And are you guys at least together in India right now? No. She’s in Ahmedabad. I am in Baroda. You flew from Canada…\n",
            "[audience laughing]\n",
            "…to India, to symbolically and romantically be together. And the minute you conducted a ceremony that binds you together for life, you f*ckin’ separated…\n",
            "[audience laughing]\n",
            "… and went to different cities.\n",
            "[man] To do the ceremony. I feel like you deserve this predicament. You’re just making bad choices in this relationship. I don’t think you understand the delicate interplay between romance and geography.\n",
            "Er…\n",
            "[audience laughing]\n",
            "[Monika] Hug people. Monika, you’re gonna go out and hug people? I’m of the same opinion as you. I feel like, the second there’s… Forget a vaccine, I feel like, a cure, we are just gonna be touching the shit out of each other.\n",
            "[all laughing]\n",
            "In a non-creepy way. Creepy is such a relative thing. I am kidding.\n",
            "[audience laughing]\n",
            "I was thinking about that the other day. If we get back to offices with social distancing, that’s gonna be terrible for the “Me, Too” guys, right? Where everybody is like six feet away. Just a lonely M.J. Akbar in the office.\n",
            "[audience laughing]\n",
            "[imitates M.J. Akbar] “You’re sexy.” Like, I don’t know… So, you know, every time I do this piece to cams in my own… private reality show that nobody’s watching, I, er… I come off as a bit of a bitch. Despite myself, I’m looking forward to today’s show. One for you, Zoom. First thing you’re going to do once the lockdown ends and we’re all allowed outside. Yell it out. Ride my bike. I mean, go riding, as in riding. I never understand when guys are like, “Dude, I rode my motorcycle.” No, you didn’t. It’s not like you showed up and your Harley was just there running wild like… [huffs, snorts] And you’re like, “Whoa, Harley, whoa.”\n",
            "[snorts]\n",
            "[audience laughing] And then your Harley Davidson took off. You’re like… And you have to jump on to it. Did I tell you, I almost bought a Harley? Like, I almost… And then I realized, “Oh, f*ck, I’m actually not an alpha.” And I think the way that you know that is the way my hands went like that. You know, guys, I’m actually not an alpha. F*ck this virus. F*ck the bat.\n",
            "[man] Play football. This isn’t one of those fantasy things where a lot of guys who can’t play football get into a room together– No, no, no. Play actual, physical football. What’s your position? What do you play? I play right-winger. You play right-winger? Cool. I’m a left-winger myself. Erm…\n",
            "[audience laughing]\n",
            "We have different goals. You know. I wanna score a goal, you know, get a penalty. You guys wanna build a statue that nobody wants.\n",
            "[audience laughing]\n",
            "[Erika] I can go out all day.\n",
            "You can go out all day? One second. What’s up, Erika, where you from? I am from Costa Rica. You’re from Costa Rica? Is there any, erm… Are there COVID patients in Costa Rica? Are there cases or not? No, we can go out all day. It’s fine? Everything’s cool? Yes, everything’s okay. Under control. Okay, I know you’re alone in the room, but can you feel the resentment from everybody else?\n",
            "[audience laughing]\n",
            "I can feel it. International, guys. [all laughing] Yeah. Hey, other Indian comedians, do you have unemployed people from COVID-free countries as your families? I think not.\n",
            "[Shreya] Go on a cruise. “Go on a cruise.” Who the f*ck said that? Shreya, you have no f*ckin’ understanding of how science works? Have you not read every news article ever… that has come out in the last… And where are you going in this cruise? I don’t know. Just to… “I just wanna be… I don’t know.” “I just wanna be stuck on a ship with some people and a virus. That’s my plan.”\n",
            "[boy] Gonna turn 16 tomorrow.\n",
            "You’re 15 years old?\n",
            "Yes. Buddy, don’t take anything I said seriously, all right?\n",
            "[audience laughing]\n",
            "Like, don’t tell anyone you were at this show, all right?\n",
            "Just…\n",
            "It’s all confidential. Don’t worry.\n",
            "Who bought the tickets?\n",
            "I did, myself.\n",
            "You have a debit card at 16?\n",
            "Yes. Are you doing child porn on the side? What the f*ck is happening? -Why do you have–\n",
            "No, not at all, sir. I’m not insinuating that you do child porn. Usually, you get recognized. Like somebody on the Zoom call be like, “Yeah, are you…\n",
            "[audience laughing]\n",
            "…Chintu69?” Er… Just a second. Just a second. Is somebody knocking on your door right now?\n",
            "[man] Yes, yes, yes.\n",
            "[all laughing]\n",
            "Is that your mom, or your girlfriend? Who is that? It’s my mom, actually. I have to take care of my sister, so she’s calling me. Buddy, go do your thing. No, I’m not going to, actually. I’m talking to you. -How old is your sister? -She’s only five years old, actually. She’ll survive, five is enough.\n",
            "[audience laughing]\n",
            "That’s fine. You don’t need to feed them or educate them. She’ll be fine. F*cker, go take care of your sister. I’m not having, like, some, er… child malnutrition on my conscience, you know. I’ve got enough. I did Mastizaade. I feel guilty enough about my life.\n",
            "[woman] From Spain.\n",
            "From Spain? Good Lord, hello. Nice to see you. It looks like you’re looking up so you’re watching me on a television? Yes. Computer’s broken. All right, so then I’ll try and make this look like it should. “Hello. How are you?” Er…\n",
            "[audience laughing]\n",
            "Is that helping you at all? Or not. Are you scared, are you nervous? Like, you know… Or are you just happy to see the f*ck out of each other? It’s weird. It’s weird because, like, er, Spanish are very sociable people. So when you meet somebody, you just kiss. Well, on the cheek. -[audience laughing] -So, its… Social distance is… We have a town called Delhi. It’s the same way, but that’s not, you know… Not quite as consensual as you guys. Yeah, that’s what happens in Delhi. [kisses]\n",
            "[in Hindi] F*ck off, motherf*cker.\n",
            "[in English] Madam, I am being Spanish, okay? Spanish. Er, I’m in Ireland right now. I’m Indian, but I am, at the moment, I’m in Ireland. You know, I guessed you were Indian from the, “Charu…” [mumbling]\n",
            "[audience laughing]\n",
            "So you’re stuck in college?\n",
            "Well, I wouldn’t call it “stuck.”\n",
            "Okay. Because we’re still, like, free to go outside on walks, like… Grocery shopping, and we can go out cycling and stuff like that. So I wouldn’t say, like, I’m stuck. All the other Indians are sitting there like…\n",
            "[in Hindi] “Motherf*cker. You f*ckin’…”\n",
            "[in English] She’s in a Spanish village, f*ckin’ drinking Prosecco. This chick is chilling and going out on her bicycle. Er, I’m doing master’s in Biomedical Engineering. Do you believe that? You believe we are headed for a vaccine September, October types? I highly doubt that, just because the whole process of actually getting a vaccine is so long. You have to, like, sleep with a Poonawalla, you know, it’s tough, er…\n",
            "[audience laughing]\n",
            "Let me tell you, that’s an inside joke. All right, cool. Just me? All right. Today is our first show for London. Er… Yay. Lond, Lond, Lond, Lond, Lond… I’m never gonna get to go to London again.\n",
            "[woman] Go to the pub.\n",
            "Go to the pub? Okay. And what’s your pub drink? What’s the first thing… What do you start the night with? Of course, a martini. A martini. Why is that “of course, a martini”? Maybe you want a beer. Like, I don’t know, if like… I’m not from, er, London. I don’t know why I got Indian when I said that shit.\n",
            "[in Indian accent] I’m not from London, son. No pubs for you. And give Kohinoor back.\n",
            "[audience laughing]\n",
            "Just got married in India on March 13th, the day the world went to shit. And we… Our marriage… Nice. So you’re having like a f*ckin’ pandemic honeymoon. This is great.\n",
            "[audience laughing]\n",
            "Is your kid gonna be… Your kid has gotta be called Corona. That’s just gotta be a thing that happens. Okay, it is, er… 7:00 a.m. on a Sunday and… I’m about to do stand-up comedy for people in New York. This is so f*ckin’ weird.\n",
            "[man] Get a haircut.\n",
            "Get a haircut. There’s a sardar on the… Jaswin, I hope that’s not you. That wasn’t you. All right, cool.\n",
            "[all laughing]\n",
            "Jaswin’s just sitting there like, “When the lockdown is done, I’m done with this Sikhism, I gotta say.”\n",
            "[audience laughing]\n",
            "[Asmita] Get my eyebrows done. Get your eyebrows done.\n",
            "[audience laughing]\n",
            "Same, girl. Need it. Okay, admin, mute her. I thought this would be good. But f*ck this, this is just a… Okay. No, let’s talk about this. Can you not do it yourself? Like, is that not a… Tweezing eyebrows is serious business. You don’t mess around with that. Why? What’s the worst case scenario? You’d just look surprised by life.\n",
            "[audience laughing]\n",
            "You’re just like… Your friends are like, “Wow, Asmita really enjoyed that Starbucks.” “You know what I mean?” “I’ve been to Starbucks with her every day for a decade, but she really got into it today, I gotta say.” In India, people are like…\n",
            "[in Hindi] “I’m not getting any food…” “Workers are migrating, this and that.”\n",
            "[in English] Here it’s like, “I can’t get my eyebrows done, and I can’t drive my car.” You f*ckin’ privileged motherf*ckers.\n",
            "[woman] No, it was a hospital birth. Erm, but it was just really odd. We didn’t have… Like, a nurse wouldn’t come to touch me or anything. So I just had to pretty much have the baby alone. That’s insane. So like, for the first time in history, the dad and the nurse have swapped. -‘Cause now the dad is doing all the work. -[audience laughing] And the nurse is just eight feet away like, “You’re doing great.”\n",
            "[audience laughing]\n",
            "The baby comes out, looks at the dad, and the baby’s like… “Is he the doctor? Why is he crying?” I heard a pretty good story about the virus today.\n",
            "[man 1] Bat ate a fruit. Dropped the fruit. A pig ate it.\n",
            "[audience laughing]\n",
            "The pig got, whatever, cut, slaughtered.\n",
            "Mmm-hmm. Went to that Wuhan wet market or whatever. Okay. Somebody bought the pig. The chef took it to a hotel. Okay. Somebody called him to say, “Compliments to the chef,” so he just wiped his hands, went and shook hands with him. Okay, so I just wanna say, if this is true, then you are a man of average intelligence and good information.\n",
            "[audience laughing]\n",
            "But if you’re f*ckin’ with me, then you’re a pure genius. I just love the confidence. [imitates man 1] “Bat. Ate the pig. Then the pig…”\n",
            "[man 2] I’m from India, stuck in Singapore.\n",
            "Plan was to call my wife here.\n",
            "Okay. But suddenly this pandemic thing, stupid thing came in. All right, cool. I know, the virus is very stupid. We’re the smart ones.\n",
            "[audience laughing]\n",
            "It is day 9,562, and the virus continues to endure. It is like Rocky, part 19.\n",
            "[flushing]\n",
            "Like, I promise you… Is somebody in the toilet? I just heard a flush.\n",
            "Did you guys f*ckin’ hear that shit?\n",
            "[audience laughing]\n",
            "You f*ckin’ hear that shit? [woman 1] Someone’s in the toilet. Someone is taking a dump during my show and it’s not me.\n",
            "[woman 2] This is epic. This is epic.\n",
            "♪ Me, ma, me, ma, me, ma, me ♪\n",
            "I’m in Madrid right now. I, er, dance flamenco here in Spain. Ooh, nice. So I’m just waiting to put on the flamenco shoes. I think you can do flamenco dancing socially distanced. Because flamenco’s like a girl who does that stuff and does the tap, tap, tap, with her feet. And then two meters away is just a guy with a guitar looking at her hornily, right? That’s basically flamenco. That’s what that is. Yeah, absolutely. Yeah. He’s like… [imitates guitar strumming] That’s it, that’s the dance. Yeah, lot of sexual tension. He’s like… [imitates guitar strumming] She’s like… [taps feet] “Stay away.” He’s like, “But I love you…” [imitates guitar strumming] “I can see it in your eyes that you want me.” She’s like, “I will hide my eyes, then.” Tsk. So, two nights ago, er… We begin with funny man Vir Das, whose neighbour threatened to cough on him. Probably the craziest night I’ve ever had in Mumbai. Er… The older gentleman sneezed on me. Er, and we’re okay. You know, we… We sorted it out. We’re friends now. But I’m just gonna tell you the story. [clicks tongue] Sneezing on someone is such a f*ckin’ intimate act, you know? It requires, like, consent and proximity, neither one of which I gave.\n",
            "[audience laughing]\n",
            "I wanted to… He was like, “You’ll be haunted,” and I didn’t respond to that. ‘Cause I’m very scared of ghosts. I’ve had, like, a ghost experience in my life. Like, it was in college where, er… [chuckles] I was… I was lying on my bed and I felt somebody sit down on the bed. There was nobody else on the bed. And I knew there was a ghost in the room. And then in my ear, I heard somebody go, [hoarse whisper] “Do you have any ganja?”\n",
            "[audience laughing]\n",
            "And I was like, “What?” And the ghost was like, “Yeah, bro, you have any stash?” “There’s no stash in heaven.” And I was like, “Why is there no stash in heaven?” And he was like, “Think about it, bro. Plants can’t grow on clouds.”\n",
            "[audience laughing]\n",
            "And I never thought about that before. Plants can’t grow on clouds. Do you know what that means? There are no vegetarians in heaven.\n",
            "[audience laughing]\n",
            "I think that’s why they call it heaven. Er…\n",
            "[audience laughing]\n",
            "And I called my lawyer in the middle of the night and she’s like, “This happened. Put it up.” And now, every single news outlet and Twitter outlet, and we’re on the f*ckin’ TV, is carrying this. And I’m positioning like I have a sense of humour about it, right? I have to get a f*ckin’ COVID test now. He pulled down his mask and he was like… He was like… Achew! Like, that’s what happened. [audience laughing] It was really sweet. Like that was the sneeze. He was like… Achew! I feel like what happened was, in his mind, he was like, “I’m gonna sneeze on him,” and his body was like, “No, you’re gonna cough on him.”\n",
            "[audience laughing]\n",
            "And then it just got combined. Your body will do that sometimes. You’re like, “I’m gonna laugh,” and your body is like, “No, you’re gonna fart and shit your pants a little bit.”\n",
            "[audience laughing]\n",
            "And I’m a comic. I know my job. I’m a comic. I will take this shit and I will write jokes about it. Everybody said, “Why didn’t you hit the man?” And the only thing I can think of is somewhere in my mind and in my body, I was like, “Don’t do this. I have a future.” I can’t get cancelled right now. I have a future. How do I even talk to a therapist about this? They’d be like… “Yeah, I’m just gonna refund this session because… they did not teach me this at the University of… Bughtown.” Bughtown? It’s such a pain in the ass to be 40 years old and now find out that you’re a f*ckin’ optimist.\n",
            "[audience laughing]\n",
            "You know, it’s like finding out you’re diabetic, you know what I mean, it’s like… “Oh, I have to change my whole life now, man. F*ck!” “I have to give up stuff that I like, sugar and self-pity.”\n",
            "[woman] No, no, but I’m American. An American who is in Delhi, so I’m from outside of the country. Nice! Yeah, so how long have you been a refugee?\n",
            "[woman] Almost three years now.\n",
            "Almost three years. And what’s your line of work?\n",
            "[woman] It’s, erm, an adjustment. I’m not okay with the locusts coming our way. I’m not okay with that. I can survive the heat, I can survive the dust storms, I can survive the politics, but I draw the line at biblical plagues. Okay, cool. Like every American, that was not the answer to my question, and too much information.\n",
            "[audience laughing]\n",
            "I had asked you what your line of work was. What do you do for a living?\n",
            "[woman] I am a travel agent. We actually got 4,000 American citizens out. We arranged flights for each one of them. Well done! Give her a round of applause. That’s well done to you. That’s great, you know? [all clapping] You know, just taking them out of India, where there are very few cases, and sending them to America.\n",
            "[audience laughing]\n",
            "[laughs] Good for you. Taking them from India where hydroxychloroquine costs 20 rupees, to America… where the medical insurance system is not flawed at all to charge you 3,000% of what the medicine actually costs.\n",
            "[woman] But I chose to stay.\n",
            "[man] Paris.\n",
            "[Vir Das] You’re in Paris? How is the situation in Paris? It’s opening up, right? There are no tourists and everywhere is free, so it’s a lot of fun. Don’t act like a f*ckin’ French person and be all egocentric about this shit. That you have a… To say shit like that, you have to have a French accent.\n",
            "[in French accent] “It is lovely, there are no tourists, I can smoke and…” “I can have this existential misery that is so typically French.”\n",
            "[in normal voice] Like, that’s… But you’re like, “No, there are no tourists, we’re liking it very much, Paris is beautiful.” Like, it’s that… You’re too friendly to pull off French smugness, I’m sorry.\n",
            "[woman 1] I’m from Melbourne. Kind of confused as to what I want to do. Okay. I’m moving on. I can’t hear you. I’m sorry. We don’t have this kind of time. A, these tickets don’t cost enough for me to get into therapy with people and shit. This is 4.99 for charity.\n",
            "[woman 2] We’re from Lagos.\n",
            "[Vir Das] You’re in Nigeria right now? What time is it right now in Africa? Those are two thumbs up, but can you tell the time? [chuckles]\n",
            "[all laughing]\n",
            "[woman 3] I’m an anthropologist. All three of us are. F*ckin’ hell! That is, like, the weirdest threesome I’ve ever seen. No. It’s just a strange… I’m just saying… that if the guy on the left developed an alcohol problem, he’d become the guy on the right. You know what I mean, it’s… Maybe I should just do the show like this, haan? Hey, guys. Er… Lockdown’s going really well. [laughs] [inhales] I will clean myself up. I am gonna make some people laugh.\n",
            "[Sharique] Sheesha.\n",
            "[Vir Das] Sheesha is not available to you? I’m staying with a kid, four months old. I cannot bring that home. And your kid does not like doing sheesha?\n",
            "[audience laughing]\n",
            "Sheesha…\n",
            "[Sharique] Definitely not. Sheesha is not a good substitute for a mother’s teat? [laughing]\n",
            "[Sharique] Definitely not. Listen, beta, we’ve got… [laughs] Listen, beta, we’ve got no lactose, but try this apple. [inhales deeply] Cough freely without being conscious that somebody’s gonna judge you. I work for an airline. It’s a good place to be coughing.\n",
            "[all laughing]\n",
            "Are you cabin crew, or do you work in, like, the offices?\n",
            "Er, the office. -All right, great. Working from home. Nice. Because you just don’t want cabin crew like… [coughing] “Veg or non-veg,” coming down the aisle.\n",
            "[audience laughing]\n",
            "[woman] I’ve come from China. I need to go back. So the first thing I need to do is go to the airport, take a flight and go back home. Okay. And when did the mental problem start?\n",
            "[audience laughing]\n",
            "You’re trying to get back into China? They shut borders to the world, like… Wow, that’s like all of their history.\n",
            "[audience laughing]\n",
            "Today is, er… hmm, a bit of a crazy day. I, er… I got featured in a magazine called Q. Much of my idols were on there. Like Seinfeld, Ellen DeGeneres and Letterman and Chappelle and Chris Rock, etc. I had like a small feature. And… I was reading the bigger features and I saw Jerry Seinfeld and Chris Rock talking to each other in a café. And they… talked about me. As in, I came up. It wasn’t like they got together… [chuckles]\n",
            "[imitates Chris Rock] “Jerry, we got to talk about Vir.” No. No, that’s… They’re talking about many things and I happen to come up. Jerry Seinfeld told Chris Rock, erm, “I saw this Indian guy, Vir Das.” “Saw some of his stuff. He’s a really funny guy.” Across the world. I’m sitting in… a 100 square-foot room in Bandra. And… in a café in New York… Jerry Seinfeld is telling Chris Rock… that I’m funny. Vir actually turns, er, 36… erm, on Sunday.\n",
            "No, f*ck off, Kavi. -I wanted to sing “Happy Birthday” to him.\n",
            "No, don’t do this shit. Please don’t do this shit. It’s embarrassing. One, two, three.\n",
            "[all singing] ♪ Happy birthday to you ♪ ♪ Happy birthday to you ♪\n",
            "It’s my birthday tomorrow. [indistinct singing] In this moment… I feel eight years old. Isha Paliwal, is this your lovely baby in the frame? No, I’m her aunt. Okay, great. That’s somebody else’s baby that you have put on camera without their consent.\n",
            "[all laughing]\n",
            "She’s just your youngest fan.\n",
            "[woman] Aw… I… How am I supposed to communicate with the baby? What do you think is gonna happen right now? Oh, well, she can reply, “Hmm, hmm, hmm…” Great, er…\n",
            "[audience laughing]\n",
            "Hello, baby. Thank you, baby, for watching this Zoom call. And I apologize, baby, for the pandemic that is out there even though you don’t f*ckin’ know what a pandemic is.\n",
            "[audience laughing]\n",
            "It’s not working, Isha. I tried, all right? F*ckin’ hell. This… Like, I’m sorry. I’m sorry. No, no. I’m very sorry. Like, I’m really good on feedback and trying to improve my craft, but this f*ckin’ baby is a bad audience member. All right. That’s what I’m gonna say. This baby is a terrible audience member and does not belong at a stand-up comedy show.\n",
            "[Yuvan] London.\n",
            "You’re in London right now? This is your lovely friend, or… This is my lovely friend. Her name’s Emily.\n",
            "Emily, you guys are seeing each other?\n",
            "Absolutely. Great, and, er, marriage on the cards? Oh, gosh, no.\n",
            "Oh…\n",
            "[all laughing]\n",
            "I’ve never had a breakup on Zoom before. I think this might be my first, guys. This is great. “Oh, f*ck, no.” He couldn’t control it, you know? Emily, you like this Indian man, yeah? Well, not right now, no.\n",
            "[all laughing]\n",
            "Oh, Yuvan, enjoy the other bedroom and\/or sofa tonight. So every time I do a show, er, when you open the door, I’m gonna show you what happens. Er… First thing when you open the door. You ready? Hang on. Let me put this viewfinder… This is the door to my study. Every time you open that door… what you see… [chuckles] is the one audience member who has watched… many, many shows. Which one of you is Smarnika, because if…\n",
            "This one.\n",
            "I said that. Because if the three of you are Smarnika, then it’s like one of those OSHO ashram things, you know what I mean? Like you are… “All women are Smarnika.”\n",
            "[all laughing]\n",
            "“You must find your inner Smarnika.” “Release your Smarnika.” Oh.\n",
            "[audience laughing]\n",
            "It’s so strange that, you know, the western sexual revolution throughout history has been driven by Indian men who, back home, nobody wanted to f*ck in the first place.\n",
            "[woman] I’m in Bombay.\n",
            "You’re in Bombay. I wanna have Thai food so I’ll go to Nara Thai. Like, they fry the betel leaf and then there’s this chutney type thing that comes along which is amazing. [laughs] What the… Okay, I’m not gonna pretend to know. What the f*ck is a betel leaf?\n",
            "[man] Paan ka patta.\n",
            "[in Hindi] You should’ve said that. “Woman from Thane.”\n",
            "[all laughing]\n",
            "[in English] “I’ll have a betel leaf.” [laughs]\n",
            "[all laughing]\n",
            "Paan patta. Paan.\n",
            "[audience laughing]\n",
            "[in Hindi] Say, “Paan,” not betel leaf.\n",
            "[in English] I love it, man. I feel… Betel leaf sounds like a safe word during sex, you know what I mean?\n",
            "[audience laughing]\n",
            "You know, couples have safe words, you know? I feel like, she sticks a finger up your ass, you’ll be like, “Betel leaf, betel leaf.”\n",
            "[audience laughing]\n",
            "And when she doesn’t get it, you’re like, “Paan ka patta, paan ka patta. Pull it out.”\n",
            "[audience laughing]\n",
            "Last show. I am, er… leaving tomorrow.\n",
            "[man] Live the same way.\n",
            "Er, Malhar Broker? -Yeah. -Okay, is your last name “Broker”? -Yeah, yeah, yeah. [laughs]\n",
            "Are you a broker? Er, no. No, of course, yeah, why would you be? That’s, yeah… Cool. No, my last name is Das and I’m not a servant. It’s okay.\n",
            "[audience laughing]\n",
            "All right. Malhar, what do you do? I’m a baker now.\n",
            "[Vir Das laughing]\n",
            "You’re called Broker Baker? Broker…\n",
            "[audience laughing]\n",
            "[Malhar] Nice name. Broker Baker. You’re just gonna go out and bake more bread for the city of…\n",
            "Mumbai?\n",
            "Ahmedabad. Of Ahmedabad? You’re… You’re in a bakery in Gujarat? Yeah.\n",
            "[audience laughing]\n",
            "I don’t think I have to say it. But I think you know what I’m thinking of. [all laughing] I’m just gonna move on really, really quickly.\n",
            "[man] I don’t know how to go to settings, to get out of privacy thing.\n",
            "[all laughing]\n",
            "Er, so, Uncle, what you do is, you go to the top right. I can’t believe… Ishaan, why do I have to explain technology to your f*ckin’ parents, dude? [audience laughing] Dad, it’s on the top right. “Dad, it’s on the top right.”\n",
            "[man] Son, I did that. I just switched in the start window. Ishaan, he did that shit.\n",
            "[all laughing]\n",
            "[man] This sounds complex. It’s very complex. Ishaan, you should’ve trained your parents better. -I’m just saying, all right? -I know, I know.\n",
            "[Vir Das] What did your mom want?\n",
            "[Aneyant] She wanted to watch it, too. Of course she’s welcome to watch it, don’t be silly. If she wants to sit down and… yeah. Er… Why am I trying to peek like I can see through your…\n",
            "[all laughing]\n",
            "Hi, Aunty. Why am I bending again? Good evening. Your son is learning a lot in this show. Er… Aunty, do you know about the girlfriend? Yeah, I… I have some, erm… You know.\n",
            "[all laughing]\n",
            "This is gonna be a silent f*ckin’ dinner at home, right? “You embarrassed the shit outta me, Mom. In front of that old-ass comedian.”\n",
            "[audience laughing]\n",
            "He’s a sweet kid. He was. [Vir Das] Cole. Nicole, er… Shiflet? Hey, I’m a healthcare worker. You’re a healthcare worker? Round of applause, immediately. Let’s do it, come on, guys. Well done. So is Dr. Sujay Jaju with us this evening?\n",
            "Yes.\n",
            "Are you here, sir? You know, I say that in public all the time, just to get a round of applause. That’s what I do. You’re isolated in Mumbai and you’re watching my show as you’re isolated in Mumbai.\n",
            "[Sujay] Yes, I am. Er, and so, this is your day off?\n",
            "It is. I’m a night shifter.\n",
            "You’re a night shifter. I work in the emergency department. I’m a paramedic. How are you feeling? I’ll begin by asking, how’s your health? Er, it’s improving each day. Like, I had a bit of fever and also stuff… COVID-related complex… But I am doing great. Yeah. You’re a paramedic in the ER? God, so you’ve had a long, I imagine, three months, right? -Yeah. For sure.\n",
            "And I imagine you’ve seen some stuff. Do you know how you got it? Where you got it?\n",
            "[Sujay] Er, actually, I was posted in COVID duty. I was posted in medicine merge for over a month. So, I might have contracted, er, the infection over there. But I know that I’ll come back stronger. Sir, I promise you two things, all right? Well, tonight’s show is entirely dedicated to you. You have my word as an artist that I will make sure it reaches the maximum amount of people that I can make it reach. It is your day off and I’m gonna give every ounce of my being to make sure that you have a good day today. Yeah, it’s definitely not as scary as we think. Actually, it’s been made a lot, er, scarier by the people.\n",
            "We all know the basics of this thing.\n",
            "[Sujay] Yeah. Wash your hands, don’t touch your face, touch your balls.\n",
            "Exactly.\n",
            "We all know the…\n",
            "[audience laughing]\n",
            "Obviously.\n",
            "[audience laughing]\n",
            "You’re in isolation. You understand as well, Doctor.\n",
            "[Sujay] Yeah. Thank you for everything you’re doing. Don’t be silly. Look, the entire show is in Hindi. Erm…\n",
            "[audience laughing]\n",
            "I, erm… Erm… Twelve years now, and it takes a lot to surprise me in this profession. We started doing these shows because we had to. For charity. And it was important and… I was a little bit cranky about it because I was like, “I don’t know if this’ll work.” For the first time, I’m looking into my audience’s houses. That’s a very weird thing to happen as an artist, you know, where the soundtrack to my life has been laughter. Every evening, I hear laughter. And I never thought that I would hear this much laughter in a lockdown. For the first time, I can see you, and your life. As opposed to you just coming and seeing me. From… everywhere in the world. So I… I thank you. Because, erm… So it’s really strange. I’ve been performing for you over a decade, but it’s like we’ve just met. You know, this show is for charity and we’ve raised money, and I hope you’ve had a good time as well, but… it is deeply… meaningful to me, to be able to practice my art form in this. You know, it’s not when there’s a cure, it’s when… It’s when you no longer fear death, I, as an artist, get to live again. And that’s very ironic. To wait on that. And not drown. Or get rusty. And the fact that we have a healthcare worker today, I… I don’t think that this could be more perfect to wrap it up, because… This is my last show. Erm… It’s true. ‘Cause we leave Bombay tomorrow. I should… I should show you the room. Why the f*ck not? This is where I’ve lived for the last few… That’s the desk. Erm… It’s kind of a wall with all my posters. That’s my books and my hats. And tomorrow… I hope the universe pays you and every healthcare worker tenfold, ten thousand fold. And thank you for everything that you do. And thank you so much for watching the show tonight, guys. I hope you had a good time.\n",
            "[soft guitar music playing]\n",
            "Bye.\n",
            "[Vir Das] Whatever it is that you want to do when the new world reopens, I hope you get to do it. And I hope you’re safe. In case you were wondering what we did the second the world reopened… Well…\n",
            "[upbeat music playing]\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Data Cleaning - Final\n"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### Importing Dependencies"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "! pip install contractions==0.0.48\n",
        "! pip install nltk\n",
        "import re\n",
        "import string\n",
        "import contractions\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk import download as nltk_download\n",
        "nltk_download(\"stopwords\")\n",
        "nltk_download('punkt')\n",
        "nltk_download('wordnet')"
      ],
      "execution_count":28,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "Requirement already satisfied: contractions==0.0.48 in \/opt\/anaconda3\/envs\/datalore-user\/lib\/python3.7\/site-packages (0.0.48)\r\n",
            "Requirement already satisfied: textsearch>=0.0.21 in \/opt\/anaconda3\/envs\/datalore-user\/lib\/python3.7\/site-packages (from contractions==0.0.48) (0.0.21)\r\n",
            "Requirement already satisfied: anyascii in \/opt\/anaconda3\/envs\/datalore-user\/lib\/python3.7\/site-packages (from textsearch>=0.0.21->contractions==0.0.48) (0.1.7)\r\n",
            "Requirement already satisfied: pyahocorasick in \/opt\/anaconda3\/envs\/datalore-user\/lib\/python3.7\/site-packages (from textsearch>=0.0.21->contractions==0.0.48) (1.4.2)\r\n",
            "Requirement already satisfied: nltk in \/opt\/anaconda3\/envs\/datalore-user\/lib\/python3.7\/site-packages (3.5)\r\n",
            "Requirement already satisfied: joblib in \/opt\/anaconda3\/envs\/datalore-user\/lib\/python3.7\/site-packages (from nltk) (1.0.1)\r\n",
            "Requirement already satisfied: click in \/opt\/anaconda3\/envs\/datalore-user\/lib\/python3.7\/site-packages (from nltk) (7.1.2)\r\n",
            "Requirement already satisfied: regex in \/opt\/anaconda3\/envs\/datalore-user\/lib\/python3.7\/site-packages (from nltk) (2020.11.13)\r\n",
            "Requirement already satisfied: tqdm in \/opt\/anaconda3\/envs\/datalore-user\/lib\/python3.7\/site-packages (from nltk) (4.57.0)\r\n"
          ],
          "output_type":"stream"
        },
        {
          "name":"stderr",
          "text":[
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     \/home\/datalore\/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to \/home\/datalore\/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to \/home\/datalore\/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "output_type":"stream"
        },
        {
          "data":{
            "text\/plain":[
              "True"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### Creating Dictionaries for Mapping"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "profanity = set(re.findall(r\"\\w+\\*\\w+\", \"\\n\\n\".join(data.iloc[:, 2]).lower()))\n",
        "profanity_dict = dict()\n",
        "for i in profanity:\n",
        "    if i[0] == \"a\":\n",
        "        profanity_dict[i] = re.sub(\"\\*\", \"s\", i)\n",
        "    elif i[0] == \"b\" or i[0] == \"n\":\n",
        "        profanity_dict[i] = re.sub(\"\\*\", \"i\", i)\n",
        "    elif i[0:2] == \"fa\":\n",
        "        profanity_dict[i] = re.sub(\"\\*\", \"g\", i)\n",
        "    else:\n",
        "        profanity_dict[i] = re.sub(\"\\*\", \"u\", i)\n",
        "del profanity\n",
        "punctuations = \"[\\\\\"+\"\\\\\".join(list(string.punctuation))+\"]\"\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "execution_count":29,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### Cleaning Data for unnecessary characters"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def clean_data(x):\n",
        "    x = x.lower()\n",
        "    for censor, uncensor in profanity_dict.items():\n",
        "        x.replace(censor, uncensor)\n",
        "    x = re.sub(r\"\\(.*?\\)\", '', re.sub(r\"\\[.*?\\]\", '', x))\n",
        "    x = x.encode(encoding=\"ascii\", errors=\"ignore\").decode()\n",
        "    x = contractions.fix(x, slang=True)\n",
        "    x = re.sub(punctuations, ' ', x)\n",
        "    x = re.sub(\"[″♬`¶¢\\‘\\—×“¡»®♫\\…\\–′’¿£♪”]\", ' ', x)\n",
        "    _RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
        "    x = _RE_COMBINE_WHITESPACE.sub(\" \", x).strip()\n",
        "    return x\n",
        "data['Cleaned_Transcript'] = data['Transcript'].apply(clean_data)"
      ],
      "execution_count":30,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### Tokenization and Lemmatization"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def tokenize_lemmatize(x):\n",
        "    tokens = [word for word in word_tokenize(x.lower()) if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    # print(tokens) \n",
        "    return tokens\n",
        "# tokenize_data = lambda x : [word for word in nltk.word_tokenize(x) if word not in stop_words]\n",
        "data['Tokens'] = data['Cleaned_Transcript'].apply(tokenize_lemmatize)"
      ],
      "execution_count":31,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Cleaning Artist Names"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def artist_name(x):\n",
        "    x = x.lower()\n",
        "    x = x.split(\":\")\n",
        "    if x[0] == \"protected\":\n",
        "        return \" \".join(x[1].split()[:2])\n",
        "    elif (\"comedy central\" in x[0]) or (\"latin history\" in x[0]):\n",
        "        return \" \".join(x[1].split()[:2])\n",
        "    elif \"gabriel\" in x[0]:\n",
        "        return \"gabriel iglesias\"\n",
        "    elif \"larry\" in x[0]:\n",
        "        return \" \".join(x[0].split()[:4])\n",
        "    elif len(x[0]) > 3:\n",
        "        return \" \".join(x[0].split()[:2])\n",
        "    else:\n",
        "        return x"
      ],
      "execution_count":32,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "data['Cleaned_Artist'] = data['Artist'].apply(artist_name)"
      ],
      "execution_count":34,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "data.head()"
      ],
      "execution_count":35,
      "outputs":[
        {
          "data":{
            "text\/html":[
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "<\/style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th><\/th>\n",
              "      <th>Artist<\/th>\n",
              "      <th>Title<\/th>\n",
              "      <th>Transcript<\/th>\n",
              "      <th>Cleaned_Transcript<\/th>\n",
              "      <th>Tokens<\/th>\n",
              "      <th>Cleaned_Artist<\/th>\n",
              "    <\/tr>\n",
              "  <\/thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0<\/th>\n",
              "      <td>Chris Rock Total Blackout: The Tamborine Exten...<\/td>\n",
              "      <td>Chris Rock Total Blackout: The Tamborine Exten...<\/td>\n",
              "      <td>[Jimmy Fallon] Were you at the, uh, White Hous...<\/td>\n",
              "      <td>were you at the uh white house party yes i was...<\/td>\n",
              "      <td>[uh, white, house, party, yes, white, house, e...<\/td>\n",
              "      <td>chris rock<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>1<\/th>\n",
              "      <td>Bo Burnham: Words, Words, Words (2010) – Trans...<\/td>\n",
              "      <td>Bo Burnham: Words, Words, Words (2010) – Trans...<\/td>\n",
              "      <td>(Cheers and applause)\\nThank you.\\n(Laughter)\\...<\/td>\n",
              "      <td>thank you when i say hey you say ho hey ho hey...<\/td>\n",
              "      <td>[thank, say, hey, say, ho, hey, ho, hey, ho, b...<\/td>\n",
              "      <td>bo burnham<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>2<\/th>\n",
              "      <td>Vir Das: Outside in – The Lockdown Special (20...<\/td>\n",
              "      <td>Vir Das: Outside in – The Lockdown Special (20...<\/td>\n",
              "      <td>[soft piano music playing]\\n[Vir Das] What you...<\/td>\n",
              "      <td>what you are about to watch was not supposed t...<\/td>\n",
              "      <td>[watch, supposed, happen, completely, unscript...<\/td>\n",
              "      <td>vir das<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>3<\/th>\n",
              "      <td>Larry the Cable Guy – Remain Seated (2020) – T...<\/td>\n",
              "      <td>Larry the Cable Guy – Remain Seated (2020) – T...<\/td>\n",
              "      <td>[Announcer] Ladies and gentlemen, Larry, The C...<\/td>\n",
              "      <td>ladies and gentlemen larry the cable guy all r...<\/td>\n",
              "      <td>[lady, gentleman, larry, cable, guy, right, th...<\/td>\n",
              "      <td>larry the cable guy<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>4<\/th>\n",
              "      <td>Craig Ferguson: Just Being Honest (2015) – Tra...<\/td>\n",
              "      <td>Craig Ferguson: Just Being Honest (2015) – Tra...<\/td>\n",
              "      <td>Watch the full show for free on YouTube\\n[bagp...<\/td>\n",
              "      <td>watch the full show for free on youtube its a ...<\/td>\n",
              "      <td>[watch, full, show, free, youtube, great, day,...<\/td>\n",
              "      <td>craig ferguson<\/td>\n",
              "    <\/tr>\n",
              "  <\/tbody>\n",
              "<\/table>\n",
              "<\/div>"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "data['Cleaned_Artist'].value_counts()[:10]"
      ],
      "execution_count":36,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "trevor noah        231\n",
              "hasan minhaj       208\n",
              "dave chappelle     191\n",
              "ali wong           186\n",
              "john mulaney       176\n",
              "amy schumer        166\n",
              "kenny sebastian    162\n",
              "kevin hart         129\n",
              "vir das            113\n",
              "kanan gill          99\n",
              "Name: Cleaned_Artist, dtype: int64"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "data.to_pickle(\"Cleaned_Transcript.pickle\")"
      ],
      "execution_count":37,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Data Cleaning test codes"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# data1 = pickle.load(open(\"Transcript.pickle\", \"rb\"))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# import re\n",
        "# a=re.sub('\\[.*?\\]', '', a)\n",
        "# a=re.sub('\\n', '', a)\n",
        "# a=re.sub('…', \" \", a)\n",
        "# a"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# _RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
        "# a = _RE_COMBINE_WHITESPACE.sub(\" \", a).strip()\n",
        "# check_elements=set(\"\\n\\n\\n\".join(data.iloc[:,2]))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# print(data.iloc[2,2])"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# def clean_data():\n",
        "#     _RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
        "#     check_elements=set(\"\\n\\n\\n\".join(data.iloc[:,2]))\n",
        "#     emoji_pattern = re.compile(\"[\"\n",
        "#                             u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "#                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "#                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "#                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "#                                u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "#                                u\"\\U00002702-\\U000027B0\"\n",
        "#                                u\"\\U00002702-\\U000027B0\"\n",
        "#                                u\"\\U000024C2-\\U0001F251\"\n",
        "#                                u\"\\U0001f926-\\U0001f937\"\n",
        "#                                u\"\\U00010000-\\U0010ffff\"\n",
        "#                                u\"\\u2640-\\u2642\"\n",
        "#                                u\"\\u2600-\\u2B55\"\n",
        "#                                u\"\\u200d\"\n",
        "#                                u\"\\u23cf\"\n",
        "#                                u\"\\u23e9\"\n",
        "#                                u\"\\u231a\"\n",
        "#                                u\"\\ufe0f\"  # dingbats\n",
        "#                                u\"\\u3030\"\n",
        "#                                \"]+\", flags=re.UNICODE)\n",
        "    \n",
        "#     d=[]\n",
        "#     k=0\n",
        "#     for i in data1['Transcript']:\n",
        "        \n",
        "#         d.append(re.sub('\\[.*?\\]', '', i))\n",
        "#         d[k]=re.sub('\\n', '', d[k])\n",
        "#         d[k]=re.sub('…', '', d[k])\n",
        "#         #d[k]=re.sub('\\(.*\\)', \" \", d[k])\n",
        "#         d[k]=emoji_pattern.sub(r'', d[k])\n",
        "#         d[k]=re.sub(r\"\\(.*?\\)\", '', d[k])\n",
        "#         d[k]=_RE_COMBINE_WHITESPACE.sub(\" \", d[k]).strip()\n",
        "#         k=k+1\n",
        "#     new_list = [re.sub(\"[″♬`¶¢\\!\\&‘\\)\\—´×\\*“\\\/\\:¡#\\.\\(»\\]\\?\\[>%®♫\\,…\\\\\\~=\\_\\–′’\\+¿\\'@\\\"£\\;♪”\\$\\-]\",' ',x) for x in d]\n",
        "#     for i in range(0,len(new_list)):\n",
        "#         new_list[i]=_RE_COMBINE_WHITESPACE.sub(\" \", new_list[i]).strip()\n",
        "#     return new_list"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# a=clean_data()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# #(a[0])\n",
        "# '''\n",
        "# check_elements=set(\"\\n\\n\\n\".join(data.iloc[:,2]))\n",
        "# #print(str(check_elements))\n",
        "# m=0\n",
        "# for j in a:\n",
        "#     elem=set(j)\n",
        "#     for element in elem:\n",
        "#         if element in check_elements:\n",
        "#             if not element.isalnum():\n",
        "               \n",
        "#                 a[m]=re.sub(element, \" \", j)\n",
        "#     print(a[m])\n",
        "#     m=m+1\n",
        "# '''\n",
        "# print(a[1])"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# print(a[1][0], a[1][-1])"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# print(re.sub(r\"\\(.*?\\)\", '', a[1]))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# data['Cleaned_Transcript']=a"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Data Visualization for NLP Project\n",
        "Please donot delete other's code, and test new codes at the end of the notebook before adding to the final cell"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import pandas as pd"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "data = pd.read_pickle(\"Cleaned_Transcript.pickle\")"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "data.columns"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "data.head()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "data = pd.read_pickle('document_term_matrix.pickle').transpose()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "data.head()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Find the top 30 words said by each comedian"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "top_dict = {}\n",
        "for c in data.columns:\n",
        "    top = data[c].sort_values(ascending=False).head(30)\n",
        "    top_dict[c]= list(zip(top.index, top.values))\n",
        "# top_dict"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "for comedian, top_words in top_dict.items():\n",
        "    print(comedian)\n",
        "    print(', '.join([word for word, count in top_words[0:14]]))\n",
        "    print('-'*100, \"\\n\")"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Look at the most common top words --> add them to the stop word list (Optional Cleaning based on requirement)"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "from collections import Counter\n",
        "\n",
        "# Let's first pull out the top 30 words for each comedian\n",
        "words = []\n",
        "for comedian in data.columns:\n",
        "    top = [word for (word, count) in top_dict[comedian]]\n",
        "    for t in top:\n",
        "        words.append(t)\n",
        "        \n",
        "words"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "    Aggregating list of most common words in routines of different comedians"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "Counter(words).most_common()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "    If more than half of the comedians have it as a top word, exclude it from the list"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "add_stop_words = [word for word, count in Counter(words).most_common() if count > 50]\n",
        "add_stop_words"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# # Let's update our document-term matrix with the new list of stop words\n",
        "# from sklearn.feature_extraction import text \n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# # Read in cleaned data\n",
        "# data_clean = pd.read_pickle('Cleaned_Transcript.pickle')\n",
        "\n",
        "# # Add new stop words\n",
        "# stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
        "\n",
        "# # Recreate document-term matrix\n",
        "# cv = CountVectorizer(stop_words=stop_words)\n",
        "# data_cv = cv.fit_transform(data_clean['Cleaned_Transcript''])\n",
        "# data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
        "# data_stop.index = data_clean.index\n",
        "\n",
        "# # Pickle it for later use\n",
        "# import pickle\n",
        "# pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
        "# data_stop.to_pickle(\"dtm_stop.pkl\")"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# from wordcloud import WordCloud\n",
        "# from sklearn.feature_extraction import text \n",
        "# stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
        "# wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
        "#                max_font_size=150, random_state=42)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.rcParams['figure.figsize'] = [16, 6]\n",
        "\n",
        "# full_names = ['ali wong', 'kenny sebastian', 'dave chappelle', 'hasan minhaj',\n",
        "#               'joe rogan', 'john mulaney']\n",
        "\n",
        "# # Create subplots for each comedian\n",
        "# for index, comedian in enumerate(data.columns):\n",
        "#     wc.generate(data_clean.Cleaned_Transcript[comedian])\n",
        "#     plt.subplot(3, 4, index+1)\n",
        "#     plt.imshow(wc, interpolation=\"bilinear\")\n",
        "#     plt.axis(\"off\")\n",
        "#     plt.title(comedian)\n",
        "    \n",
        "# plt.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Visualising the top 15 artist based on their profanity"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "data_bad_words = data.transpose()[['fucking', 'fuck', 'shit']]\n",
        "data_profanity = pd.concat([data_bad_words.fucking + data_bad_words.fuck, data_bad_words.shit], axis=1)\n",
        "data_profanity.columns = ['f_word', 's_word']\n",
        "data_profanity = data_profanity.sort_values(ascending=False, by=[\"f_word\", \"s_word\"]).head(15)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "plt.rcParams['figure.figsize'] = [12, 10]\n",
        "\n",
        "for i, comedian in enumerate(data_profanity.index):\n",
        "    x = data_profanity.f_word.loc[comedian]\n",
        "    y = data_profanity.s_word.loc[comedian]\n",
        "    plt.scatter(x, y, color='blue')\n",
        "    plt.text(x+1.5, y+0.5, comedian, fontsize=10)\n",
        "    \n",
        "plt.title('Number of Bad Words Used in Routine', fontsize=20)\n",
        "plt.xlabel('Number of F Bombs', fontsize=15)\n",
        "plt.ylabel('Number of S Words', fontsize=15)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Text Generator\n",
        "* Please donot modify the code below. \n",
        "* The code will not execute in datalore, use Google Colab with GPU\n",
        "* *Will only work on Google Colab*"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Import dependencies"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from google.colab import files"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Setup Enironment with GPU"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "from tensorflow.python.keras import backend as K\n",
        "config = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 8} )\n",
        "sess = tf.compat.v1.Session(config=config) \n",
        "K.set_session(sess)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### LSTM Model"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "class TextGenerator:\n",
        "    def __init__(self, vocab_size, seq_length, tokenizer):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_length = seq_length\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        model = None\n",
        "        history = None\n",
        "    \n",
        "    def build(self):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(self.vocab_size, 100, input_length=self.seq_length))\n",
        "        model.add(LSTM(128, return_sequences=True))\n",
        "        model.add(LSTM(128))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(self.vocab_size, activation='softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        self.model = model\n",
        "        self.model.summary()\n",
        "    \n",
        "    def fit(self, X, y, batch_size=256, epochs=100, validation_split=0):\n",
        "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "        self.history = self.model.fit(X, y, batch_size=batch_size, epochs=epochs, \n",
        "                            validation_split=validation_split, callbacks=[callback])\n",
        "    \n",
        "    def plot_model(self):\n",
        "        # summarize history for accuracy\n",
        "        print()\n",
        "        plt.plot(self.history.history['accuracy'])\n",
        "        plt.plot(self.history.history['val_accuracy'])\n",
        "        plt.title('model accuracy')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "        # summarize history for loss\n",
        "        print()\n",
        "        plt.plot(self.history.history['loss'])\n",
        "        plt.plot(self.history.history['val_loss'])\n",
        "        plt.title('model loss')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        plt.show() \n",
        "\n",
        "    def generate_texts(self, seed_text, n_words):\n",
        "        print(\"_\"*150)\n",
        "        text = []\n",
        "        print(seed_text, end='\\n\\n')\n",
        "        for _ in range(n_words):\n",
        "            encoded = self.tokenizer.texts_to_sequences([seed_text])[0]\n",
        "            encoded = pad_sequences([encoded], maxlen=self.seq_length, truncating='pre')\n",
        "            y_predict = np.argmax(self.model.predict(encoded), axis=-1)\n",
        "\n",
        "            predicted_word = ''\n",
        "            for word, index in self.tokenizer.word_index.items():\n",
        "                if index == y_predict:\n",
        "                    predicted_word = word\n",
        "                    break\n",
        "            seed_text = seed_text + ' ' + predicted_word\n",
        "            text.append(predicted_word)\n",
        "        return ' '.join(text)\n",
        "    \n",
        "    def save_model(self, name=\"model.h5\"):\n",
        "        self.model.save(name)\n",
        "        print(\"\\n\\nSaved Successfully: {}\".format(name))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Driver Function"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def train_create_model(artist, dataset, input_length=100, stop_words=False):\n",
        "    data = dataset[artist == dataset['Artist']].reset_index().drop(\"index\", axis=1)\n",
        "    data_list = []\n",
        "    # with stop words\n",
        "    if stop_words:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        data['Tokens'] = data['Transcript'].apply(lambda x : [lemmatizer.lemmatize(w) for w in word_tokenize(x.lower())])\n",
        "        # data.iloc[:, 2].apply(lambda x : data_list.extend(x))\n",
        "        name = artist + \"_stop_word.h5\"\n",
        "    else:\n",
        "        # data.iloc[:, 2].apply(lambda x : data_list.extend(x))\n",
        "        name = artist + \".h5\"\n",
        "    data.iloc[:, 2].apply(lambda x : data_list.extend(x))\n",
        "    print(\"Length of Data List: \", len(data_list))\n",
        "    data = data_list\n",
        "    del data_list\n",
        "    \n",
        "    n_words = len(data)\n",
        "    unique_words = len(set(data))\n",
        "    print('Total Words: %d' % n_words)\n",
        "    print('Unique Words: %d\\n' % unique_words)\n",
        "\n",
        "    length = input_length + 1\n",
        "    lines = []\n",
        "    for i in range(length, n_words):\n",
        "        seq = data[i - length : i]\n",
        "        line = ' '.join(seq)\n",
        "        lines.append(line)\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    sequences = tokenizer.texts_to_sequences(lines)\n",
        "    sequences = np.array(sequences)\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    word_2_index = tokenizer.word_index\n",
        "\n",
        "    X, y = sequences[:, :-1], sequences[:, -1]\n",
        "    y = to_categorical(y, vocab_size)\n",
        "\n",
        "    seq_length = X.shape[1]\n",
        "    print(\"X shape:\", X.shape)\n",
        "    print(\"y shape:\", y.shape)\n",
        "\n",
        "    model = TextGenerator(vocab_size, seq_length, tokenizer)\n",
        "    model.build()\n",
        "    model.fit(X, y, batch_size=256, epochs=100, validation_split=0.1)\n",
        "    model.plot_model()\n",
        "    print(model.generate_texts(lines[np.random.randint(0, len(lines))], 50))\n",
        "    model.save_model(name)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Prepare Data"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "data = pd.read_pickle(\"\/content\/Cleaned_Transcript.pickle\")\n",
        "data = data[['Cleaned_Artist', 'Cleaned_Transcript', 'Tokens']]\n",
        "data.columns = ['Artist', 'Transcript', 'Tokens']"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "print(data['Artist'].value_counts()[:10])\n",
        "# print(\"_\"*50)\n",
        "# print(data['Cleaned_Artist'].value_counts()[:10])"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "##### Trevor Noah"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"trevor noah\", data, stop_words=False)\n",
        "files.download('trevor noah.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"trevor noah\", data, stop_words=True)\n",
        "files.download('trevor noah_stop_word.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "##### Hasan Minhaj"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"hasan minhaj\", data, stop_words=False)\n",
        "files.download('hasan minhaj')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"hasan minhaj\", data, stop_words=True)\n",
        "files.download('hasan minhaj_stop_word.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "##### Dave Chappelle"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"dave chappelle\", data, stop_words=False)\n",
        "files.download('dave chappelle.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"dave chappelle\", data, stop_words=True)\n",
        "files.download('dave chappelle_stop_word.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "##### Ali Wong"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"ali wong\", data, stop_words=False)\n",
        "files.download('ali wong.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"ali wong\", data, stop_words=True)\n",
        "files.download('ali wong_stop_word.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "##### John Mulaney"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"john mulaney\", data, stop_words=False)\n",
        "files.download('john mulaney.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"john mulaney\", data, stop_words=True)\n",
        "files.download('john mulaney_stop_word.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "##### Amy Schumer"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"amy schumer\", data, stop_words=False)\n",
        "files.download('amy schumer.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"amy schumer\", data, stop_words=True)\n",
        "files.download('amy schumer_stop_word.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "##### Kenny Sebastian"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"kenny sebastian\", data, stop_words=False)\n",
        "files.download('kenny sebastian.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"kenny sebastian\", data, stop_words=True)\n",
        "files.download('kenny sebastian_stop_word.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "##### Kevin Hart"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"kevin hart\", data, stop_words=False)\n",
        "files.download('kevin hart.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"kevin hart\", data, stop_words=True)\n",
        "files.download('kevin hart_stop_word.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "##### Vir Das"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"vir das\", data, stop_words=False)\n",
        "files.download('vir das.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"vir das\", data, stop_words=True)\n",
        "files.download('vir das_stop_word.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "##### Kanan Gill"
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"kanan gill\", data, stop_words=False)\n",
        "files.download('kanan gill.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_create_model(\"kanan gill\", data, stop_words=True)\n",
        "files.download('kanan gill_stop_word.h5')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    }
  ],
  "metadata":{
    
  },
  "nbformat":4,
  "nbformat_minor":0
}